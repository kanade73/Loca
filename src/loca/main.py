import time
import argparse

# --- ã‚³ã‚¢æ©Ÿèƒ½ ---
from loca.core.llm_client import chat_with_llm, stream_chat_with_llm, extract_json_from_text, estimate_tokens
from loca.core.prompts import get_system_prompt
from loca.core.memory import MemoryManager
from loca.core.router import route_command
from loca.core.executor import execute_action

# --- ãƒ„ãƒ¼ãƒ« ---
from loca.tools.web_search import search_web

# --- UI ---
from loca.ui.header import print_header
from loca.ui.display import console, print_thought, print_error, get_user_input
from rich.live import Live
from rich.markdown import Markdown

# ==========================================
# ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç®¡ç†ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦æº¢ã‚Œé˜²æ­¢ï¼‰
# ==========================================
MAX_EXCHANGES = 30       # 1ã‚»ãƒƒã‚·ãƒ§ãƒ³ä¸­ã®æœ€å¤§ã‚„ã‚Šã¨ã‚Šå›æ•°
MAX_MESSAGES = 60        # messagesãƒªã‚¹ãƒˆã®ä¸Šé™ï¼ˆã“ã‚Œã‚’è¶…ãˆãŸã‚‰å¤ã„ã‚‚ã®ã‚’æ¨ã¦ã‚‹ï¼‰

def trim_messages(messages: list) -> list:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã‚’é˜²ãã ã‘ã®å®‰å…¨è£…ç½®ã€‚"""
    if len(messages) <= MAX_MESSAGES:
        return messages
    
    # system_prompt (messages[0]) + ç›´è¿‘ã®ã‚„ã‚Šã¨ã‚Šã ã‘æ®‹ã™
    trimmed = [messages[0]] + messages[-(MAX_MESSAGES - 1):]
    console.print(f"[dim]ğŸ“ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ•´ç†: å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ‡ã‚Šæ¨ã¦ã¾ã—ãŸ ({len(messages)} â†’ {len(trimmed)})[/dim]")
    return trimmed

# ==========================================
# ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—
# ==========================================
def main(model_name: str, provider: str):
    # ãƒ˜ãƒƒãƒ€ãƒ¼è¡¨ç¤º
    print_header(model_name=f"{model_name} ({provider.upper()})")
    
    # è¨˜æ†¶ç®¡ç†ã‚¯ãƒ©ã‚¹ã®åˆæœŸåŒ–
    memory = MemoryManager()
    
    sys_prompt = get_system_prompt()
    messages = [sys_prompt]
    
    if "<project_guidelines>" in sys_prompt["content"]:
        console.print("[bold cyan]ğŸ§  Locaã®è¨˜æ†¶(Loca.md)ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸï¼[/bold cyan]\n")
        
    needs_user_input = True 
    auto_mode = False
    is_ask_mode = False
    exchange_count = 0  # LLMå‘¼ã³å‡ºã—å›æ•°ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼

    while True:
        # --- ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ç®¡ç† ---
        messages = trim_messages(messages)
        
        # --- ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ•ã‚§ãƒ¼ã‚º ---
        if needs_user_input:
            try:
                user_input = get_user_input()
            except (KeyboardInterrupt, EOFError):
                console.print("\n[dim]Shutting down agent...[/dim]")
                break

            # ã‚³ãƒãƒ³ãƒ‰ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°
            route_result, auto_mode, exchange_count = route_command(
                user_input, messages, memory,
                model_name=model_name, provider=provider,
                auto_mode=auto_mode, exchange_count=exchange_count,
            )
            
            if route_result.should_exit:
                break
            if route_result.handled:
                needs_user_input = True
                continue
            
            is_ask_mode = route_result.is_ask_mode
                
        # --- AIæ€è€ƒãƒ•ã‚§ãƒ¼ã‚º ---
        # äº¤æ›å›æ•°ãƒã‚§ãƒƒã‚¯
        exchange_count += 1
        if exchange_count > MAX_EXCHANGES:
            console.print(f"\n[bold yellow]âš ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¸Šé™ ({MAX_EXCHANGES}å›) ã«é”ã—ã¾ã—ãŸã€‚ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã™ã€‚[/bold yellow]")
            console.print("[dim]æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚[/dim]\n")
            messages.clear()
            messages.append(get_system_prompt())
            exchange_count = 0
            needs_user_input = True
            continue
        
        # ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã®æ¦‚ç®—è¡¨ç¤º
        token_count = estimate_tokens(messages)
        console.print(f"[dim]ğŸ“Š Tokens: ~{token_count} | Exchange: {exchange_count}/{MAX_EXCHANGES}[/dim]")
        
        start_time = time.time()
        
        # /ask ãƒ¢ãƒ¼ãƒ‰: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°è¡¨ç¤º
        if is_ask_mode:
            raw_text = ""
            
            # ã¾ãšé€šå¸¸ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ï¼ˆweb_searchã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã®åˆ¤å®šã®ãŸã‚ï¼‰
            with console.status("[bold cyan]AI is thinking...", spinner="dots"):
                response_data = chat_with_llm(messages, model_name=model_name, provider=provider, is_ask_mode=True)
            
            raw_text = response_data.get("raw_response", "")
            parsed_json = extract_json_from_text(raw_text)
            
            if parsed_json and parsed_json.get("action") == "search_web":
                query = parsed_json.get("query", "")
                console.print(f"\n[bold cyan]ğŸ” æ¤œç´¢ä¸­:[/bold cyan] {query}")
                
                with console.status("[bold yellow]Webã‚’æ¤œç´¢ã—ã€å›ç­”ã‚’ç”Ÿæˆä¸­...", spinner="dots"):
                    search_result = search_web(query)
                    messages.append({"role": "assistant", "content": raw_text})
                    messages.append({"role": "user", "content": f"æ¤œç´¢çµæœ:\n{search_result}\n\nã“ã®çµæœã‚’è¸ã¾ãˆã¦ã€æœ€åˆã®è³ªå•ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã§ç›´æ¥ç­”ãˆã¦ãã ã•ã„ã€‚"})
                
                # æ¤œç´¢çµæœã‚’è¸ã¾ãˆãŸå›ç­”ã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§è¡¨ç¤º
                raw_text = ""
                console.print()
                with Live("", console=console, refresh_per_second=8) as live:
                    for chunk in stream_chat_with_llm(messages, model_name=model_name, provider=provider):
                        raw_text += chunk
                        live.update(Markdown(raw_text))
            else:
                # é€šå¸¸ã®/askå›ç­”: ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§å†åº¦ç”Ÿæˆ
                raw_text = ""
                console.print()
                with Live("", console=console, refresh_per_second=8) as live:
                    for chunk in stream_chat_with_llm(messages, model_name=model_name, provider=provider):
                        raw_text += chunk
                        live.update(Markdown(raw_text))

            elapsed_time = time.time() - start_time
            console.print(f"\n[dim]â±ï¸ Answered in {elapsed_time:.1f}s[/dim]")
            messages.append({"role": "assistant", "content": raw_text})
            needs_user_input = True
            continue

        # é€šå¸¸ãƒ¢ãƒ¼ãƒ‰: JSONãƒ¬ã‚¹ãƒãƒ³ã‚¹ï¼ˆéã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ï¼‰
        with console.status("[bold cyan]AI is thinking...", spinner="dots"):
            response_data = chat_with_llm(messages, model_name=model_name, provider=provider, is_ask_mode=False)
        
        # JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã®è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ï¼ˆ1å›ã¾ã§ï¼‰
        if "error" in response_data and response_data.get("error") == "JSON_PARSE_ERROR":
            console.print("[dim]ğŸ”„ JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ä¸­...[/dim]")
            messages.append({"role": "assistant", "content": response_data.get("raw_response", "")})
            messages.append({"role": "user", "content": "ã‚ãªãŸã®å‰ã®å¿œç­”ã¯JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æŒ‡å®šã•ã‚ŒãŸJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å†åº¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"})
            with console.status("[bold cyan]AI is retrying...", spinner="dots"):
                response_data = chat_with_llm(messages, model_name=model_name, provider=provider, is_ask_mode=False)

        if "error" in response_data:
            print_error("ã†ã¾ãè§£é‡ˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            console.print(f"[dim]è©³ç´°: {response_data.get('raw_response', response_data)}[/dim]")
            needs_user_input = True
            continue

        thought = response_data.get("thought", "æ€è€ƒãƒ—ãƒ­ã‚»ã‚¹ãªã—")
        action = response_data.get("action", "none")
        args = response_data.get("args", {})
        elapsed_time = time.time() - start_time

        console.print(f"[dim]â±ï¸ Thought completed in {elapsed_time:.1f}s[/dim]")
        print_thought(thought)

        # --- ã‚¢ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œãƒ•ã‚§ãƒ¼ã‚º ---
        result_output, should_kill = execute_action(action, args, auto_mode)
        
        if should_kill:
            messages.append({"role": "user", "content": "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã“ã®å‡¦ç†ã‚’å¼·åˆ¶çµ‚äº†ã—ã¾ã—ãŸã€‚çµ¶å¯¾ã«ã“ã‚Œä»¥ä¸Šä½•ã‚‚ã›ãšã€ã™ãã« Action: none ã‚’è¿”ã—ã¦å¾…æ©ŸçŠ¶æ…‹ã«æˆ»ã£ã¦ãã ã•ã„ã€‚"})
            needs_user_input = True
            continue

        if action != "none":
            messages.append({"role": "assistant", "content": f"```json\n{{\"action\": \"{action}\", \"args\": {args}}}\n```"})
            messages.append({"role": "user", "content": f"å®Ÿè¡Œçµæœ:\n```\n{result_output}\n```\næ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚å®Œå…¨ã«é”æˆã•ã‚ŒãŸå ´åˆã®ã¿ action: none ã«ã—ã¦ãã ã•ã„ã€‚"})
            if result_output:
                console.print(f"\n[bold]Action Result:[/bold]\n[dim]{result_output}[/dim]\n")
            needs_user_input = False 
        else:
            messages.append({"role": "assistant", "content": f"Thought: {thought}\n(Action: none)"})
            console.print("[bold green]âœ… ã‚¿ã‚¹ã‚¯å®Œäº†[/bold green]\n")
            needs_user_input = True 

def cli():
    import loca.config as config
    parser = argparse.ArgumentParser(description="Loca - Autonomous AI Coding Assistant")
    parser.add_argument("-p", "--provider", type=str, default=config.DEFAULT_PROVIDER, choices=["ollama", "openai", "anthropic", "gemini"], help="LLMã®ãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼")
    parser.add_argument("-m", "--model", type=str, default=config.DEFAULT_MODEL, help="ä½¿ç”¨ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å")
    
    args = parser.parse_args()
    main(model_name=args.model, provider=args.provider)

if __name__ == "__main__":
    cli()