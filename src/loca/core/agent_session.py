"""
AgentSession: ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã¨ãƒ•ãƒ©ã‚°ç®¡ç†ã‚’ã‚«ãƒ—ã‚»ãƒ«åŒ–ã—ãŸã‚¯ãƒ©ã‚¹ã€‚
ä»¥å‰ã® main.py ã«æ•£åœ¨ã—ã¦ã„ãŸçŠ¶æ…‹ã‚’ä¸€å…ƒç®¡ç†ã—ã€ãƒ†ã‚¹ãƒˆå®¹æ˜“æ€§ã‚’é«˜ã‚ã‚‹ã€‚
"""
import json
import time

from loca.core.llm_client import (
    chat_with_tools,
    chat_with_llm,
    stream_chat_with_llm,
    extract_json_from_text,
    estimate_tokens,
)
from loca.core.prompts import get_system_prompt, get_agent_system_prompt
from loca.core.memory import MemoryManager
from loca.core.router import route_command
from loca.core.executor import create_default_registry, backup_manager
from loca.core.tool_registry import ToolRegistry
from loca.tools.web_search import search_web
from loca.ui.header import print_header
from loca.ui.display import console, print_thought, print_error, get_user_input
from rich.live import Live
from rich.markdown import Markdown

MAX_EXCHANGES = 30
MAX_MESSAGES = 60


def _trim_messages(messages: list) -> list:
    """ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã‚’é˜²ãå®‰å…¨è£…ç½®ã€‚"""
    if len(messages) <= MAX_MESSAGES:
        return messages
    trimmed = [messages[0]] + messages[-(MAX_MESSAGES - 1):]
    console.print(
        f"[dim]ğŸ“ ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ•´ç†: å¤ã„ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã‚’åˆ‡ã‚Šæ¨ã¦ã¾ã—ãŸ ({len(messages)} â†’ {len(trimmed)})[/dim]"
    )
    return trimmed


class AgentSession:
    """
    1ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†ã®ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆçŠ¶æ…‹ã‚’ä¿æŒã—ã€ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã‚’å®Ÿè¡Œã™ã‚‹ã‚¯ãƒ©ã‚¹ã€‚

    Attributes:
        model_name: ä½¿ç”¨ã™ã‚‹LLMãƒ¢ãƒ‡ãƒ«å
        provider: LLMãƒ—ãƒ­ãƒã‚¤ãƒ€ãƒ¼ï¼ˆollama/openai/anthropic/geminiï¼‰
        messages: ä¼šè©±å±¥æ­´
        auto_mode: True ã®ã¨ãç¢ºèªãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
        exchange_count: ç¾ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®LLMå‘¼ã³å‡ºã—å›æ•°
        needs_user_input: True ã®ã¨ãæ¬¡ã®ãƒ«ãƒ¼ãƒ—ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å¾…ã¤
        is_ask_mode: /ask ã‚³ãƒãƒ³ãƒ‰ã«ã‚ˆã‚‹ä¼šè©±ãƒ¢ãƒ¼ãƒ‰ã‹å¦ã‹
    """

    def __init__(self, model_name: str, provider: str):
        self.model_name = model_name
        self.provider = provider
        self.memory = MemoryManager()
        self.registry: ToolRegistry = create_default_registry()
        self.messages: list = []
        self.auto_mode: bool = False
        self.exchange_count: int = 0
        self.needs_user_input: bool = True
        self.is_ask_mode: bool = False
        self._reset_messages()

    # ------------------------------------------------------------------
    # ãƒ‘ãƒ–ãƒªãƒƒã‚¯ API
    # ------------------------------------------------------------------

    def run(self) -> None:
        """ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã‚’é–‹å§‹ã™ã‚‹ã€‚"""
        print_header(model_name=f"{self.model_name} ({self.provider.upper()})")

        if "<project_guidelines>" in self.messages[0]["content"]:
            console.print("[bold cyan]ğŸ§  Locaã®è¨˜æ†¶(Loca.md)ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã—ãŸï¼[/bold cyan]\n")

        while True:
            self.messages = _trim_messages(self.messages)

            if self.needs_user_input:
                should_continue = self._handle_user_input()
                if not should_continue:
                    break
                if self.needs_user_input:
                    # ã‚³ãƒãƒ³ãƒ‰ã¨ã—ã¦å‡¦ç†æ¸ˆã¿ â†’ æ¬¡ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å¾…ã¤
                    continue

            self._run_ai_step()

    # ------------------------------------------------------------------
    # ãƒ—ãƒ©ã‚¤ãƒ™ãƒ¼ãƒˆãƒ¡ã‚½ãƒƒãƒ‰
    # ------------------------------------------------------------------

    def _reset_messages(self) -> None:
        """ä¼šè©±å±¥æ­´ã¨ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆã™ã‚‹ã€‚"""
        self.messages = [get_agent_system_prompt()]
        self.exchange_count = 0

    def _handle_user_input(self) -> bool:
        """
        ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å—ã‘å–ã‚Šã€ãƒ«ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ã™ã‚‹ã€‚
        False ã‚’è¿”ã™ã¨ãƒ¡ã‚¤ãƒ³ãƒ«ãƒ¼ãƒ—ã‚’çµ‚äº†ã™ã‚‹ã€‚
        """
        try:
            user_input = get_user_input()
        except (KeyboardInterrupt, EOFError):
            console.print("\n[dim]Shutting down agent...[/dim]")
            return False

        route_result, self.auto_mode, self.exchange_count = route_command(
            user_input, self.messages, self.memory,
            model_name=self.model_name, provider=self.provider,
            auto_mode=self.auto_mode, exchange_count=self.exchange_count,
        )

        if route_result.should_exit:
            return False

        if route_result.handled:
            # /clear ç­‰ã§ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ãŒæ›¸ãæ›ã‚ã£ãŸå ´åˆã€FCç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç¢ºå®Ÿã«å¾©å…ƒã™ã‚‹
            if self.messages:
                self.messages[0] = get_agent_system_prompt()
            self.needs_user_input = True
            return True

        self.is_ask_mode = route_result.is_ask_mode
        self.needs_user_input = False
        return True

    def _run_ai_step(self) -> None:
        """AIæ€è€ƒãƒ•ã‚§ãƒ¼ã‚ºã‚’å®Ÿè¡Œã—ã€needs_user_input ã‚’æ›´æ–°ã™ã‚‹ã€‚"""
        self.exchange_count += 1
        if self.exchange_count > MAX_EXCHANGES:
            console.print(
                f"\n[bold yellow]âš ï¸ ã‚»ãƒƒã‚·ãƒ§ãƒ³ã®ä¸Šé™ ({MAX_EXCHANGES}å›) ã«é”ã—ã¾ã—ãŸã€‚"
                "ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒªã‚»ãƒƒãƒˆã—ã¾ã™ã€‚[/bold yellow]"
            )
            console.print("[dim]æ–°ã—ã„ã‚¿ã‚¹ã‚¯ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚[/dim]\n")
            self._reset_messages()
            self.needs_user_input = True
            return

        token_count = estimate_tokens(self.messages)
        console.print(
            f"[dim]ğŸ“Š Tokens: ~{token_count} | Exchange: {self.exchange_count}/{MAX_EXCHANGES}[/dim]"
        )

        start_time = time.time()

        if self.is_ask_mode:
            self._run_ask_step(start_time)
        else:
            self._run_agent_step(start_time)

    def _run_ask_step(self, start_time: float) -> None:
        """
        /ask ãƒ¢ãƒ¼ãƒ‰: 1å›ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°å‘¼ã³å‡ºã—ã§å›ç­”ã™ã‚‹ã€‚

        ã€2é‡å‘¼ã³å‡ºã—è§£æ¶ˆã€‘
        æ—§å®Ÿè£…: chat_with_llm() (éã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°) â†’ search_webåˆ¤å®š â†’ stream_chat_with_llm() (å†åº¦å‘¼ã³å‡ºã—)
        æ–°å®Ÿè£…: stream_chat_with_llm() ã§ä¸€åº¦ã ã‘ãƒãƒƒãƒ•ã‚¡ã«åé›† â†’ search_webåˆ¤å®š â†’ å¿…è¦ãªã‚‰2å›ç›®
        """
        # ask ãƒ¢ãƒ¼ãƒ‰ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ router.py ãŒ messages[0] ã«è¨­å®šæ¸ˆã¿
        raw_text = ""

        # 1å›ã®ã‚¹ãƒˆãƒªãƒ¼ãƒŸãƒ³ã‚°ã§ãƒãƒƒãƒ•ã‚¡ã«åé›†ï¼ˆspinner ã‚’è¡¨ç¤ºã—ãªãŒã‚‰ï¼‰
        with console.status("[bold cyan]AI is thinking...", spinner="dots"):
            for chunk in stream_chat_with_llm(
                self.messages, model_name=self.model_name, provider=self.provider
            ):
                raw_text += chunk

        parsed_json = extract_json_from_text(raw_text)

        if parsed_json and parsed_json.get("action") == "search_web":
            # Webæ¤œç´¢ãŒå¿…è¦ãªå ´åˆã®ã¿2å›ç›®ã®å‘¼ã³å‡ºã—ã‚’è¡Œã†
            query = parsed_json.get("query", "")
            console.print(f"\n[bold cyan]ğŸ” æ¤œç´¢ä¸­:[/bold cyan] {query}")

            with console.status("[bold yellow]Webã‚’æ¤œç´¢ã—ã€å›ç­”ã‚’ç”Ÿæˆä¸­...", spinner="dots"):
                search_result = search_web(query)
                self.messages.append({"role": "assistant", "content": raw_text})
                self.messages.append({
                    "role": "user",
                    "content": (
                        f"æ¤œç´¢çµæœ:\n{search_result}\n\n"
                        "ã“ã®çµæœã‚’è¸ã¾ãˆã¦ã€æœ€åˆã®è³ªå•ã«ãƒãƒ¼ã‚¯ãƒ€ã‚¦ãƒ³ã§ç›´æ¥ç­”ãˆã¦ãã ã•ã„ã€‚"
                    ),
                })

            raw_text = ""
            console.print()
            with Live("", console=console, refresh_per_second=8) as live:
                for chunk in stream_chat_with_llm(
                    self.messages, model_name=self.model_name, provider=self.provider
                ):
                    raw_text += chunk
                    live.update(Markdown(raw_text))
        else:
            # é€šå¸¸ã®å›ç­”: ãƒãƒƒãƒ•ã‚¡æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆã‚’ Markdown ã§ãƒ¬ãƒ³ãƒ€ãƒªãƒ³ã‚°
            console.print()
            console.print(Markdown(raw_text))

        elapsed_time = time.time() - start_time
        console.print(f"\n[dim]â±ï¸ Answered in {elapsed_time:.1f}s[/dim]")
        self.messages.append({"role": "assistant", "content": raw_text})
        self.needs_user_input = True
        self.is_ask_mode = False
        # ask ãƒ¢ãƒ¼ãƒ‰çµ‚äº†å¾Œã€ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¾©å…ƒ
        self.messages[0] = get_agent_system_prompt()

    def _run_agent_step(self, start_time: float) -> None:
        """
        é€šå¸¸ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰: Function Calling ã§ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’æ±ºå®šãƒ»å®Ÿè¡Œã™ã‚‹ã€‚
        Function Calling éå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã®å ´åˆã¯ JSON ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨ã€‚
        """
        # ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆãƒ¢ãƒ¼ãƒ‰ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ç¢ºå®Ÿã«ã‚»ãƒƒãƒˆ
        self.messages[0] = get_agent_system_prompt()

        tools = self.registry.openai_schemas()

        with console.status("[bold cyan]AI is thinking...", spinner="dots"):
            response_data = chat_with_tools(
                self.messages, tools,
                model_name=self.model_name, provider=self.provider,
            )

        # Function Calling éå¯¾å¿œãƒ¢ãƒ‡ãƒ«ã¸ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if response_data.get("error") == "NO_TOOL_CALL":
            console.print(
                "[dim]âš ï¸ Function CallingãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚JSONãƒ¢ãƒ¼ãƒ‰ã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã—ã¾ã™ã€‚[/dim]"
            )
            response_data = self._fallback_json_call()

        if "error" in response_data:
            print_error("ã†ã¾ãè§£é‡ˆã§ãã¾ã›ã‚“ã§ã—ãŸã€‚")
            console.print(f"[dim]è©³ç´°: {response_data.get('raw_response', response_data)}[/dim]")
            self.needs_user_input = True
            return

        thought = response_data.get("thought", "")
        action = response_data.get("action", "none")
        args = response_data.get("args", {})
        elapsed_time = time.time() - start_time

        console.print(f"[dim]â±ï¸ Thought completed in {elapsed_time:.1f}s[/dim]")
        print_thought(thought)

        result_output, should_kill = self.registry.execute(action, args, self.auto_mode)

        if should_kill:
            self.messages.append({
                "role": "user",
                "content": (
                    "ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒã“ã®å‡¦ç†ã‚’å¼·åˆ¶çµ‚äº†ã—ã¾ã—ãŸã€‚"
                    "çµ¶å¯¾ã«ã“ã‚Œä»¥ä¸Šä½•ã‚‚ã›ãšã€ã™ãã« none ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—ã¦å¾…æ©ŸçŠ¶æ…‹ã«æˆ»ã£ã¦ãã ã•ã„ã€‚"
                ),
            })
            self.needs_user_input = True
            return

        if action != "none":
            # ä¼šè©±å±¥æ­´ã¯ãƒ†ã‚­ã‚¹ãƒˆå½¢å¼ã§ä¿å­˜ï¼ˆFC/JSONãƒ¢ãƒ¼ãƒ‰ä¸¡æ–¹ã§å†åˆ©ç”¨ã§ãã‚‹ï¼‰
            self.messages.append({
                "role": "assistant",
                "content": f"```json\n{{\"action\": \"{action}\", \"args\": {json.dumps(args, ensure_ascii=False)}}}\n```",
            })
            self.messages.append({
                "role": "user",
                "content": (
                    f"å®Ÿè¡Œçµæœ:\n```\n{result_output}\n```\n"
                    "æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚å®Œå…¨ã«é”æˆã•ã‚ŒãŸå ´åˆã®ã¿ none ãƒ„ãƒ¼ãƒ«ã‚’å‘¼ã³å‡ºã—ã¦ãã ã•ã„ã€‚"
                ),
            })
            if result_output:
                console.print(f"\n[bold]Action Result:[/bold]\n[dim]{result_output}[/dim]\n")
            self.needs_user_input = False
        else:
            self.messages.append({
                "role": "assistant",
                "content": f"Thought: {thought}\n(Action: none)",
            })
            console.print("[bold green]âœ… ã‚¿ã‚¹ã‚¯å®Œäº†[/bold green]\n")
            self.needs_user_input = True

    def _fallback_json_call(self) -> dict:
        """
        Function Calling éå¯¾å¿œãƒ¢ãƒ‡ãƒ«ç”¨ã® JSON ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã€‚
        æ—§æ¥ã® chat_with_llm() ã‚’ä½¿ç”¨ã—ã€JSONãƒ‘ãƒ¼ã‚¹ã‚’è©¦ã¿ã‚‹ã€‚
        """
        # JSON ãƒ¢ãƒ¼ãƒ‰ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ä¸€æ™‚çš„ã«åˆ‡ã‚Šæ›¿ãˆ
        fc_prompt = self.messages[0]
        self.messages[0] = get_system_prompt()

        with console.status("[bold cyan]AI is retrying (JSON mode)...", spinner="dots"):
            response_data = chat_with_llm(
                self.messages, model_name=self.model_name, provider=self.provider, is_ask_mode=False
            )

        # JSONãƒ‘ãƒ¼ã‚¹å¤±æ•—æ™‚ã¯ã‚‚ã†1å›ãƒªãƒˆãƒ©ã‚¤
        if response_data.get("error") == "JSON_PARSE_ERROR":
            console.print("[dim]ğŸ”„ JSONãƒ‘ãƒ¼ã‚¹ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è‡ªå‹•ãƒªãƒˆãƒ©ã‚¤ä¸­...[/dim]")
            self.messages.append({"role": "assistant", "content": response_data.get("raw_response", "")})
            self.messages.append({
                "role": "user",
                "content": "ã‚ãªãŸã®å‰ã®å¿œç­”ã¯JSONã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚æŒ‡å®šã•ã‚ŒãŸJSONãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆã§å†åº¦å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚",
            })
            with console.status("[bold cyan]AI is retrying...", spinner="dots"):
                response_data = chat_with_llm(
                    self.messages, model_name=self.model_name, provider=self.provider, is_ask_mode=False
                )

        # FC ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å¾©å…ƒ
        self.messages[0] = fc_prompt
        return response_data
